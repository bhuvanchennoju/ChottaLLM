{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens in train: 0.01M\n",
      "number of tokens in valid: 0.01M\n",
      "number of tokens in test: 0.01M\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(npysplit):\n",
    "    data = np.load(npysplit)\n",
    "    return data\n",
    "\n",
    "data_dir = \"/n/projects/kc2819/projects/ChotaLLM/data/wikitext2\"\n",
    "trn_path, val_path, tst_path = data_dir + \"/wikitext-2-raw-v1_train_000000.npy\", data_dir + \"/wikitext-2-raw-v1_validation_000000.npy\", data_dir + \"/wikitext-2-raw-v1_test_000000.npy\"\n",
    "train_data = load_data(trn_path)\n",
    "valid_data = load_data(val_path)\n",
    "test_data = load_data(tst_path)\n",
    "\n",
    "\n",
    "print(f'number of tokens in train: {len(train_data)/10e6}M')\n",
    "print(f'number of tokens in valid: {len(valid_data)/10e6}M')\n",
    "print(f'number of tokens in test: {len(test_data)/10e6}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([50256, 50256,   796,   569, 18354,  7496, 17740,  6711,   796,\n",
       "          220,   198, 50256, 50256,  2311,    73, 13090,   645,   569,\n",
       "        18354,  7496,   513,  1058,   791, 47398, 17740,   357,  4960,\n",
       "         1058, 10545,   230,    99,   161,   254,   112,  5641, 44444,\n",
       "         9202, 25084, 24440, 12675, 11839,    18,   837,  6578,   764,\n",
       "          569, 18354,  7496,   286,   262, 30193,   513,  1267,   837,\n",
       "         8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
       "         2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,\n",
       "           31,  2712,  2008,   983,  4166,   416, 29490,   290,  6343,\n",
       "           13, 44206,   329,   262, 14047, 44685,   764, 28728,   287,\n",
       "         3269,  2813,   287,  2869,   837,   340,   318,   262,  2368,\n",
       "          983], dtype=uint16),\n",
       " array([50256, 50256,   796,  5199,   347,  2852,   353,   796,   220,\n",
       "          198, 50256, 50256,  5199,   347,  2852,   353,   318,   281,\n",
       "         3594,  2646,   837,  5581,   290, 21421,  8674,   764,   679,\n",
       "          550,   257,  8319,  2488,    12,    31, 20495,  2597,   319,\n",
       "          262,  5581,  2168,   383,  3941,   287,  4751,   764,   770,\n",
       "          373,  3940,   416,   257, 20495,  2597,   287,   262,   711,\n",
       "         2332,   684,  3194,   416, 11288, 37072,   837,   543,   373,\n",
       "         6157,   287,  5878,   379,   262,  8111,  3078, 15752,   764,\n",
       "          679,   550,   257,  8319,  2597,   287,   262,  5581,  2168,\n",
       "         8974,  1757,  1024,   276,   287,  6244,   764,   554,  5472,\n",
       "          347,  2852,   353, 11406,   257,  2597,   355,   366, 13854,\n",
       "          366], dtype=uint16),\n",
       " array([50256, 50256,   796,  8074, 20272,  9106,  3876,   385,   796,\n",
       "          220,   198, 50256, 50256,  8074, 20272,  9106,  3876,   385,\n",
       "          837,  1900,   355,   262,  3427, 43657,   393,  2219, 43657,\n",
       "          837,   318,   257,  4693,   286, 26573,   276, 43657,   422,\n",
       "          262, 10183, 10596, 10692,   837, 19517,  6896,   290,  3354,\n",
       "          286,   262,  2619,  6896,   764,   632,   318,  7173,  3519,\n",
       "          284,   262,  1605, 43657,   837,   367,    13, 45630, 41141,\n",
       "          764,   632,   743,  1663,   284,   257,  4129,   286,  3126,\n",
       "        12067,   357,  1987,   287,  1267,   290,   257,  2347,   286,\n",
       "          718, 37075,   357,  1511, 18360,  1267,   837,   290, 13062,\n",
       "          257, 39089,  5166,   286, 28421,   764,   554,  1204,   837,\n",
       "          262], dtype=uint16))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:100], test_data[:100], valid_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/n/projects/kc2819/projects/ChotaLLM\")\n",
    "\n",
    "from src.dataUtils import CustomDataset, CustomDataloader, customTokenizer\n",
    "\n",
    "batch_size = 5\n",
    "block_size = 32\n",
    "embedding_size = 12\n",
    "\n",
    "tokenizer = customTokenizer()\n",
    "\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_encoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/dev.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/dev.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdir\u001b[39m(get_encoding(\u001b[39m'\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_encoding' is not defined"
     ]
    }
   ],
   "source": [
    "dir(get_encoding('gpt2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|endoftext|> = Valkyria Chronicles III = \n",
      "<|endoftext|><|endoftext|> Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "<|endoftext|> The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "<|endoftext|> It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
      "<|endoftext|><|endoftext|> = = Gameplay = = \n",
      "<|endoftext|><|endoftext|> As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \n",
      "<|endoftext|> The game 's battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \n",
      "<|endoftext|> Troops are divided into five classes : Scouts , Shocktroopers , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned\n"
     ]
    }
   ],
   "source": [
    "seq = train_data[:1000]\n",
    "print(tokenizer.decode_np(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(trn_path, block_size= block_size)\n",
    "valid_dataset = CustomDataset(val_path, block_size= block_size)\n",
    "test_dataset = CustomDataset(tst_path, block_size= block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125 3125 3125\n",
      "train (tensor([50256, 50256,   796,   569, 18354,  7496, 17740,  6711,   796,   220,\n",
      "          198, 50256, 50256,  2311,    73, 13090,   645,   569, 18354,  7496,\n",
      "          513,  1058,   791, 47398, 17740,   357,  4960,  1058, 10545,   230,\n",
      "           99,   161]), tensor([50256,   796,   569, 18354,  7496, 17740,  6711,   796,   220,   198,\n",
      "        50256, 50256,  2311,    73, 13090,   645,   569, 18354,  7496,   513,\n",
      "         1058,   791, 47398, 17740,   357,  4960,  1058, 10545,   230,    99,\n",
      "          161,   254])) \n",
      " valid (tensor([50256, 50256,   796,  8074, 20272,  9106,  3876,   385,   796,   220,\n",
      "          198, 50256, 50256,  8074, 20272,  9106,  3876,   385,   837,  1900,\n",
      "          355,   262,  3427, 43657,   393,  2219, 43657,   837,   318,   257,\n",
      "         4693,   286]), tensor([50256,   796,  8074, 20272,  9106,  3876,   385,   796,   220,   198,\n",
      "        50256, 50256,  8074, 20272,  9106,  3876,   385,   837,  1900,   355,\n",
      "          262,  3427, 43657,   393,  2219, 43657,   837,   318,   257,  4693,\n",
      "          286, 26573])) \n",
      " test (tensor([50256, 50256,   796,  5199,   347,  2852,   353,   796,   220,   198,\n",
      "        50256, 50256,  5199,   347,  2852,   353,   318,   281,  3594,  2646,\n",
      "          837,  5581,   290, 21421,  8674,   764,   679,   550,   257,  8319,\n",
      "         2488,    12]), tensor([50256,   796,  5199,   347,  2852,   353,   796,   220,   198, 50256,\n",
      "        50256,  5199,   347,  2852,   353,   318,   281,  3594,  2646,   837,\n",
      "         5581,   290, 21421,  8674,   764,   679,   550,   257,  8319,  2488,\n",
      "           12,    31]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/projects/kc2819/projects/ChotaLLM/src/dataUtils.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_ids = tensor(inputs_ids,dtype = int64)\n",
      "/n/projects/kc2819/projects/ChotaLLM/src/dataUtils.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = tensor(labels,dtype = int64)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "print(f'train', train_dataset[0],'\\n', f'valid', valid_dataset[0],'\\n', f'test', test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test the dataloaders\n",
    "train_loader = CustomDataloader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = CustomDataloader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = CustomDataloader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[  764,  5501,  4843,   286,   262,  1492,   705,    82,  7110, 10069,\n",
       "             257,  1180, 28953,   319,   262, 32022,  2066,   705,    82,  1964,\n",
       "             290,  1919,  2858,   837,  1613,   290,  1944,   764,   220,   198,\n",
       "           50256, 41969],\n",
       "          [  837, 10807,   371,  9250,   373,  4137,   284,  1011,  1630,   286,\n",
       "             262,  3074,   764,  1550,  3945,   718,   837,   339,  1908,   257,\n",
       "            8766,  3512,   329, 16908,   286,   262, 24375,   284,  8599,   309,\n",
       "            4728,   837],\n",
       "          [  315,   292,   764,   220,   198, 50256, 11450,   837, 15987,   746,\n",
       "            4910,   373,  8879,   416,  1194, 30529,    84,  2584,   272, 30968,\n",
       "             837,   290,   788,   416, 30311, 37399,   378,   837,   508, 29209,\n",
       "             340,   284],\n",
       "          [  714,  5198,   284,   649,  1938,   981, 26645,   262,  6393,  6805,\n",
       "             286,   262,  2168,   705, 11327,   764,   383, 15064,  3341,   547,\n",
       "            3066,  2402,  1903,   287,  2478,   764,   383,  2095,  9824,   547,\n",
       "            1760,   416],\n",
       "          [  198, 50256, 39775,   468,   257, 10153,   257,  1178,  8331,   890,\n",
       "             319,   262,  1364,  1735,   286,   607, 22531,   290, 19353,   837,\n",
       "             262,  2728,   286,   543,  6150, 43286,   284,   262,  1171,  1566,\n",
       "             257,  3648]]),\n",
       "  tensor([[ 5501,  4843,   286,   262,  1492,   705,    82,  7110, 10069,   257,\n",
       "            1180, 28953,   319,   262, 32022,  2066,   705,    82,  1964,   290,\n",
       "            1919,  2858,   837,  1613,   290,  1944,   764,   220,   198, 50256,\n",
       "           41969,   389],\n",
       "          [10807,   371,  9250,   373,  4137,   284,  1011,  1630,   286,   262,\n",
       "            3074,   764,  1550,  3945,   718,   837,   339,  1908,   257,  8766,\n",
       "            3512,   329, 16908,   286,   262, 24375,   284,  8599,   309,  4728,\n",
       "             837,   220],\n",
       "          [  292,   764,   220,   198, 50256, 11450,   837, 15987,   746,  4910,\n",
       "             373,  8879,   416,  1194, 30529,    84,  2584,   272, 30968,   837,\n",
       "             290,   788,   416, 30311, 37399,   378,   837,   508, 29209,   340,\n",
       "             284,   262],\n",
       "          [ 5198,   284,   649,  1938,   981, 26645,   262,  6393,  6805,   286,\n",
       "             262,  2168,   705, 11327,   764,   383, 15064,  3341,   547,  3066,\n",
       "            2402,  1903,   287,  2478,   764,   383,  2095,  9824,   547,  1760,\n",
       "             416,   371],\n",
       "          [50256, 39775,   468,   257, 10153,   257,  1178,  8331,   890,   319,\n",
       "             262,  1364,  1735,   286,   607, 22531,   290, 19353,   837,   262,\n",
       "            2728,   286,   543,  6150, 43286,   284,   262,  1171,  1566,   257,\n",
       "            3648, 46584]])),\n",
       " (tensor([[ 4912,   373,  7042,   287, 12122,   284,  1037, 10568, 37990,  1871,\n",
       "           24424,  4393,   837, 40586,   837,  1230,  5942,   837,   290,   584,\n",
       "            4609,  4671,   764,   383, 26138,  1138,   290,  4987,   319,  2458,\n",
       "             284,  1956],\n",
       "          [  513,  2488,    11,    31, 12877,  8573,   764,   383,  1353,  4867,\n",
       "             286,   262,  2524, 10874,   286,   257,  2168,   286, 10730, 48520,\n",
       "            5755,  3212,  7396,  2029, 26837,    88,  1877,  4447,   764,   383,\n",
       "            1688, 10959],\n",
       "          [  286,   326,   614,   764,   679,  7482,   257, 21933,   287, 12122,\n",
       "             878, 30774,   314,  9301,   683,   329,  4856,  3967,   329, 13181,\n",
       "             440,  4825,   357,   625,   262,  3753,  1267,  5010,   837, 21089,\n",
       "             929,   305],\n",
       "          [  796,   796,   220,   198, 50256, 50256,   366, 19061,   366,   373,\n",
       "             530,   286,   262,   717,  8339,  4166,   329,  7326,  6202,   319,\n",
       "             257, 17349, 22343,   837,  1863,   351,   366, 12149,  3205,   366,\n",
       "             290,   366],\n",
       "          [ 2323,  1241,   837,  3501,   262,  4645,   257,  2472,  6001,   286,\n",
       "           12713,  3625,   422,  4220,   284,  1353,  1081,   636,   286,   262,\n",
       "            1628,   837,   314,  2488,    12,    31,   718,  2327,   373, 36405,\n",
       "             284,  2291]]),\n",
       "  tensor([[  373,  7042,   287, 12122,   284,  1037, 10568, 37990,  1871, 24424,\n",
       "            4393,   837, 40586,   837,  1230,  5942,   837,   290,   584,  4609,\n",
       "            4671,   764,   383, 26138,  1138,   290,  4987,   319,  2458,   284,\n",
       "            1956,  2488],\n",
       "          [ 2488,    11,    31, 12877,  8573,   764,   383,  1353,  4867,   286,\n",
       "             262,  2524, 10874,   286,   257,  2168,   286, 10730, 48520,  5755,\n",
       "            3212,  7396,  2029, 26837,    88,  1877,  4447,   764,   383,  1688,\n",
       "           10959,   286],\n",
       "          [  326,   614,   764,   679,  7482,   257, 21933,   287, 12122,   878,\n",
       "           30774,   314,  9301,   683,   329,  4856,  3967,   329, 13181,   440,\n",
       "            4825,   357,   625,   262,  3753,  1267,  5010,   837, 21089,   929,\n",
       "             305, 41037],\n",
       "          [  796,   220,   198, 50256, 50256,   366, 19061,   366,   373,   530,\n",
       "             286,   262,   717,  8339,  4166,   329,  7326,  6202,   319,   257,\n",
       "           17349, 22343,   837,  1863,   351,   366, 12149,  3205,   366,   290,\n",
       "             366, 10898],\n",
       "          [ 1241,   837,  3501,   262,  4645,   257,  2472,  6001,   286, 12713,\n",
       "            3625,   422,  4220,   284,  1353,  1081,   636,   286,   262,  1628,\n",
       "             837,   314,  2488,    12,    31,   718,  2327,   373, 36405,   284,\n",
       "            2291,  1440]])),\n",
       " (tensor([[ 3303,   837,   355,   880,   355,   257, 16826,   284,  6306,   351,\n",
       "            1729,  2488,    12,    31,  4569, 18527,  5107,   764, 39440,  1023,\n",
       "             779,  1479, 18527,   764,   220,   198, 50256, 39440,   396, 16125,\n",
       "           12655,  1022],\n",
       "          [  329,  2211, 11946, 41416,   561,   307,  2716,  1626,   262,   717,\n",
       "            2063,   286,  1946,   764, 49953,   262,   886,   286,  2211,   837,\n",
       "           13004,  8721, 42529,  1364,   262,  4097,   351,   645,  7468,   290,\n",
       "             645,  2912],\n",
       "          [ 3263,   796,   796,   796,   220,   198, 50256, 50256,   383, 14104,\n",
       "            4153,  2488,    12,    31, 27417,   260,  3439,  2488,    13,    31,\n",
       "             718, 12067,   357,  1478,  2488,    13,    31,   657,   287,  1267,\n",
       "            5994,  6073],\n",
       "          [  565,  5643,   547,  3170,   319, 13510,   393,  6953, 23644,    82,\n",
       "             837,   290,  6861,   416, 38733,  6953,  8328,  9763,   416,  6546,\n",
       "           13510,   458, 15230,   764,  7931,   565,  5643,   547,   991,   852,\n",
       "            3170,   351],\n",
       "          [46058,   286,   262,  2254,  1398,   837,   290,   373,   546,   284,\n",
       "            1844,   262, 23116, 18289,   837,   281, 13097,  1486,  5150,   416,\n",
       "             262, 14023, 33475,  1757,  7651, 16528,   764,   383,  4479,   373,\n",
       "             635,  2615]]),\n",
       "  tensor([[  837,   355,   880,   355,   257, 16826,   284,  6306,   351,  1729,\n",
       "            2488,    12,    31,  4569, 18527,  5107,   764, 39440,  1023,   779,\n",
       "            1479, 18527,   764,   220,   198, 50256, 39440,   396, 16125, 12655,\n",
       "            1022, 26833],\n",
       "          [ 2211, 11946, 41416,   561,   307,  2716,  1626,   262,   717,  2063,\n",
       "             286,  1946,   764, 49953,   262,   886,   286,  2211,   837, 13004,\n",
       "            8721, 42529,  1364,   262,  4097,   351,   645,  7468,   290,   645,\n",
       "            2912,   422],\n",
       "          [  796,   796,   796,   220,   198, 50256, 50256,   383, 14104,  4153,\n",
       "            2488,    12,    31, 27417,   260,  3439,  2488,    13,    31,   718,\n",
       "           12067,   357,  1478,  2488,    13,    31,   657,   287,  1267,  5994,\n",
       "            6073,  6541],\n",
       "          [ 5643,   547,  3170,   319, 13510,   393,  6953, 23644,    82,   837,\n",
       "             290,  6861,   416, 38733,  6953,  8328,  9763,   416,  6546, 13510,\n",
       "             458, 15230,   764,  7931,   565,  5643,   547,   991,   852,  3170,\n",
       "             351, 13510],\n",
       "          [  286,   262,  2254,  1398,   837,   290,   373,   546,   284,  1844,\n",
       "             262, 23116, 18289,   837,   281, 13097,  1486,  5150,   416,   262,\n",
       "           14023, 33475,  1757,  7651, 16528,   764,   383,  4479,   373,   635,\n",
       "            2615,   257]])))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader)), next(iter(valid_loader)), next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 32]) torch.Size([5, 32])\n",
      "torch.Size([5, 32]) torch.Size([5, 32])\n",
      "torch.Size([5, 32]) torch.Size([5, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trn_batch = next(iter(train_loader))\n",
    "print(trn_batch[0].shape, trn_batch[1].shape) # batch_size, block_size\n",
    "\n",
    "tst_batch = next(iter(test_loader))\n",
    "print(tst_batch[0].shape, tst_batch[1].shape)\n",
    "\n",
    "val_batch = next(iter(valid_loader))\n",
    "print(val_batch[0].shape, val_batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n",
      "Parameter containing:\n",
      "tensor([[ 1.1204, -0.1094, -1.1308],\n",
      "        [ 0.3742,  0.3927,  0.4792],\n",
      "        [-0.3982, -0.4998, -0.7896],\n",
      "        [-0.4837,  0.4450,  0.4109],\n",
      "        [-0.8308, -1.6025, -0.0689],\n",
      "        [ 0.8746, -0.6552,  0.5417],\n",
      "        [ 0.4448,  0.4127, -0.6123],\n",
      "        [ 0.1343, -0.5025, -0.8294],\n",
      "        [-1.9889, -0.1112,  0.2929],\n",
      "        [-0.2769, -0.4240, -0.1411]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# simple lookup table of index - num_embeddings with each index having embedding_dim vector representation\n",
    "emb_layer = nn.Embedding(num_embeddings = 10, embedding_dim = 3) # vocab_size, embedding_size\n",
    "\n",
    "print(emb_layer.weight.shape)\n",
    "\n",
    "print(emb_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3742,  0.3927,  0.4792],\n",
      "        [-0.3982, -0.4998, -0.7896],\n",
      "        [-0.4837,  0.4450,  0.4109]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "input_indicies = torch.tensor([1,2,3])\n",
    "map = emb_layer(input_indicies)\n",
    "print(map)\n",
    "print(map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[[ 0.3742,  0.3927,  0.4792],\n",
      "         [-0.3982, -0.4998, -0.7896],\n",
      "         [-0.4837,  0.4450,  0.4109]],\n",
      "\n",
      "        [[-0.8308, -1.6025, -0.0689],\n",
      "         [ 0.8746, -0.6552,  0.5417],\n",
      "         [ 0.4448,  0.4127, -0.6123]],\n",
      "\n",
      "        [[ 0.1343, -0.5025, -0.8294],\n",
      "         [-1.9889, -0.1112,  0.2929],\n",
      "         [-0.2769, -0.4240, -0.1411]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "input_indicies = torch.tensor([[1,2,3],[4,5,6],[7,8,9]]) # batch of 3, with three tokens each\n",
    "print(input_indicies.shape)\n",
    "map = emb_layer(input_indicies)\n",
    "print(map)\n",
    "print(map.shape)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50256, 5303, 703, 389, 345, 1804], [50256, 72, 716, 1804, 922]]\n",
      "6\n",
      "[[50256, 5303, 703, 389, 345, 1804], [50256, 72, 716, 1804, 922, 0]]\n",
      "tensor([[50256,  5303,   703,   389,   345,  1804],\n",
      "        [50256,    72,   716,  1804,   922,     0]])\n"
     ]
    }
   ],
   "source": [
    "# lets create the sentence to embedding for the model\n",
    "\n",
    "sent_batch = [\n",
    "    'hi how are you doing',\n",
    "    'i am doing good',\n",
    "]\n",
    "\n",
    "tok_b = [tokenizer.encode(s) for s in sent_batch]\n",
    "print(tok_b)\n",
    "\n",
    "# lets pad the tokens\n",
    "max_len = max([len(t) for t in tok_b])\n",
    "print(max_len)\n",
    "\n",
    "padded = [t + [0]*(max_len - len(t)) for t in tok_b]\n",
    "\n",
    "print(padded)\n",
    "\n",
    "\n",
    "input_tensor = torch.tensor(padded)\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputblock(nn.Module): # inherit the base class\n",
    "\n",
    "    def __init__(self,vocab_size,block_size,embedding_size):\n",
    "        super(inputblock, self).__init__()   ## always need to call this super constructor to initialize the base class\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embedding_size) # vocab_size * embedding_size look up table\n",
    "        # back in the day we used to add sin and cos positional encodings here, but now we just use embeddings for that\n",
    "        # this is because the model can learn the positional encodings as well\n",
    "        self.pos_emb = nn.Embedding(block_size, embedding_size) \n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        sentence: 'hi how are you doing' ---> | gpt2 encoding | ---> 50256(eot),  5303,   703,   389,   345,  1804 --> this token vector (batch)\n",
    "\n",
    "        batch.shape = (B,T) ---> batch_size, block_size/context window\n",
    "\n",
    "                  | -> token embedding - (B,T,C) lookup table                         |\n",
    "        batch --> |                                                                   | -> pos + tok = (B,T,C) positional + token embeddings \n",
    "                  | -> positional embedding - (T,C) learnable positional embeddings   |                 positional embeddings casted to batch size\n",
    "                  \n",
    "        \n",
    "        \"\"\"\n",
    "        B,C = batch.shape\n",
    "        tok_emb = self.tok_emb(batch) # B,T,C\n",
    "        pos = torch.arange(C).expand(B,C).to(batch.device)\n",
    "        pos_emb = self.pos_emb(pos) # T,C\n",
    "        return tok_emb + pos_emb    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 6\n"
     ]
    }
   ],
   "source": [
    "B,T = input_tensor.shape\n",
    "print(B,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 12])\n"
     ]
    }
   ],
   "source": [
    "i2tr = inputblock(vocab_size = tokenizer.get_vocab_size(), block_size = T, embedding_size = embedding_size)\n",
    "\n",
    "out = i2tr(input_tensor)\n",
    "print(out.shape) # B,T,C = 2, 6, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attentionhead(nn.Module):\n",
    "\n",
    "    def __init__(self,emb_size,head_size):\n",
    "        super(attentionhead, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        assert emb_size % head_size == 0, 'embedding size must be divisible by head size'\n",
    "\n",
    "        # q,k,v embeddings \n",
    "        self.qkv = nn.Linear(emb_size, 3 * head_size,bias=False) # (B,T,hc) -> (B,T,hc) x 3              \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(emb_size,emb_size))) # lower triangular matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.size()\n",
    "        q,k,v = self.qkv(x).split(self.head_size, dim = 2) # I have to split this properly at head_size to unpack \n",
    "                                                           # (B,T,hc) -> (B,T,hc) x 3 -> (B,T,hc) x 3\n",
    "        \n",
    "        # w = softmax(qk^T/sqrt(d_k))v\n",
    "        w = q @ k.transpose(1,2) * (C ** -0.5) # (B,T,hc) x (B,hc,T) -> (B,T,T) \n",
    "        w = w.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # mask the upper triangular matrix\n",
    "        w = F.softmax(w,dim = 2) # (B,T,T)\n",
    "        return w @ v # (B,T,T) x (B,T,hc) -> (B,T,hc)\n",
    "    \n",
    "\n",
    "\n",
    "class attensionblock(nn.Module):\n",
    "\n",
    "    def __init__(self,emb_size,head_size):\n",
    "        super(attensionblock, self).__init__()\n",
    "        assert emb_size % head_size == 0, 'embedding size must be devisable by head size'\n",
    "        self.head_size = head_size\n",
    "        self.emb_size = emb_size\n",
    "        self.n_heads = self.emb_size // self.head_size\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([attentionhead(emb_size,head_size) for _ in range(self.n_heads)])\n",
    "\n",
    "        self.fc = nn.Linear(emb_size,emb_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.size()\n",
    "        w = torch.cat([attn_head(x) for attn_head in self.attention_heads],dim = 2) # (B,T,hc) x n_heads -> (B,T,hc*n_heads)\n",
    "        return self.fc(w) # (B,T,hc*n_heads) -> (B,T,C)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4])\n",
      "torch.Size([2, 6, 12])\n"
     ]
    }
   ],
   "source": [
    "attn = attentionhead(emb_size = embedding_size, head_size = 4)\n",
    "attn_out = attn(out)\n",
    "print(attn_out.shape) # B,T,C = 2, 6, 12\n",
    "\n",
    "attn_block = attensionblock(emb_size = embedding_size, head_size = 4)\n",
    "attn_block_out = attn_block(out)\n",
    "print(attn_block_out.shape) # B,T,C = 2, 6, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
